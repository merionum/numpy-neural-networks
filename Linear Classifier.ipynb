{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Linear classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrDWcdzmRrFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as io\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_WaQNB1RrFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dataset import load_svhn, random_split_train_val\n",
        "from gradient_check import check_gradient\n",
        "from metrics import multiclass_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbtktcOgRrFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_for_linear_classifier(train_X, test_X):\n",
        "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
        "    \n",
        "    # Subtract mean\n",
        "    mean_image = np.mean(train_flat, axis = 0)\n",
        "    train_flat -= mean_image\n",
        "    test_flat -= mean_image\n",
        "    \n",
        "    # Add another channel with ones as a bias term\n",
        "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
        "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
        "    return train_flat_with_ones, test_flat_with_ones\n",
        "    \n",
        "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
        "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
        "# Split train into train and val\n",
        "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJu90goRrF9",
        "colab_type": "text"
      },
      "source": [
        "Gradient check: \n",
        "\n",
        "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQDLoJRCRrGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(preds):\n",
        "    dim = preds.ndim\n",
        "    if dim == 1:\n",
        "        c = np.max(preds)\n",
        "        return np.exp(preds - c) / np.sum(np.exp(preds - c))\n",
        "    sm = []\n",
        "    log = 0\n",
        "    for p in preds:\n",
        "        c = np.max(p)\n",
        "        sm.append(np.exp(p - c) / np.sum(np.exp(p - c)))\n",
        "    return np.stack(sm, axis=0)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(predictions, targets):\n",
        "    if predictions.ndim == 1:\n",
        "        return -np.log(predictions[targets])\n",
        "    N = predictions.shape[0]\n",
        "    ce = (-np.sum(np.log(predictions[np.arange(N), targets.flatten()]))) / N\n",
        "    return ce\n",
        "\n",
        "\n",
        "def softmax_with_cross_entropy(predictions, target_index):\n",
        "    ndim = predictions.ndim\n",
        "    sm = softmax(predictions)\n",
        "    loss = cross_entropy_loss(sm, target_index)\n",
        "    dprediction = np.zeros_like(sm)\n",
        "    if ndim == 1:\n",
        "        dprediction[target_index] = 1\n",
        "    else:\n",
        "        dprediction[np.arange(dprediction.shape[0]),\n",
        "                    target_index.flatten()] = 1\n",
        "    dprediction = sm - dprediction\n",
        "    return loss, dprediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1k3vs9gRrG-",
        "colab_type": "code",
        "colab": {},
        "outputId": "3e1f0772-b9e2-49f0-b64b-5fc183f234cf"
      },
      "source": [
        "# TODO Implement combined function or softmax and cross entropy and produce gradient\n",
        "loss, grad = softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
        "check_gradient(lambda x: softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5761168847658291 0.5761168847651099\n",
            "-0.7880584423829146 -0.7880584423691771\n",
            "0.21194155761708544 0.2119415576151695\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eAkTjviNRrHK",
        "colab_type": "code",
        "colab": {},
        "outputId": "7bfe9d8a-b2ad-43e0-cccf-51d3374f1113"
      },
      "source": [
        "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
        "# Test batch_size = 1\n",
        "num_classes = 4\n",
        "batch_size = 1\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Test batch_size = 3\n",
        "num_classes = 4\n",
        "batch_size = 3\n",
        "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
        "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
        "check_gradient(lambda x: softmax_with_cross_entropy(x, target_index), predictions)\n",
        "\n",
        "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
        "probs = softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
        "assert np.all(np.isclose(probs[:, 0], 1.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.05406459218899647 0.05406459219203796\n",
            "0.39948630465030277 0.39948630465858054\n",
            "0.39948630465030277 0.3994863046474783\n",
            "-0.853037201489602 -0.8530372014869946\n",
            "Gradient check passed!\n",
            "0.08714431874203257 0.08714431878331651\n",
            "0.6439142598879724 0.6439142599123926\n",
            "0.23688281808991013 0.23688281807210385\n",
            "-0.967941396719915 -0.967941396723404\n",
            "0.13447071068499755 0.13447071069982997\n",
            "0.13447071068499755 0.13447071069982997\n",
            "0.36552928931500245 0.3655292892812411\n",
            "-0.6344707106849976 -0.6344707106809011\n",
            "-0.6005136953496972 -0.6005136953479706\n",
            "0.39948630465030277 0.39948630465858054\n",
            "0.14696279851039795 0.14696279850845428\n",
            "0.05406459218899647 0.054064592180935726\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53JCOgOhRrHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_softmax(X, W, target_index):\n",
        "    '''\n",
        "    Performs linear classification and returns loss and gradient over W\n",
        "\n",
        "    Arguments:\n",
        "      X, np array, shape (num_batch, num_features) - batch of images\n",
        "      W, np array, shape (num_features, classes) - weights\n",
        "      target_index, np array, shape (num_batch) - index of target classes\n",
        "\n",
        "    Returns:\n",
        "      loss, single value - cross-entropy loss\n",
        "      gradient, np.array same shape as W - gradient of weight by loss\n",
        "\n",
        "    '''\n",
        "    m = X.shape[0]\n",
        "    predictions = np.dot(X, W)\n",
        "    loss, grad = softmax_with_cross_entropy(predictions, target_index)\n",
        "    dW = np.dot(X.T, grad)\n",
        "    return loss, dW"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJUJqtLCRrHr",
        "colab_type": "code",
        "colab": {},
        "outputId": "3daf3929-7144-466e-9c7a-449935c991ce"
      },
      "source": [
        "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
        "batch_size = 2\n",
        "num_classes = 2\n",
        "num_features = 3\n",
        "np.random.seed(42)\n",
        "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
        "print(W)\n",
        "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
        "print(X)\n",
        "target_index = np.ones(batch_size, dtype=np.int)\n",
        "loss, dW = linear_softmax(X, W, target_index)\n",
        "check_gradient(lambda w: linear_softmax(X, w, target_index), W)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.  2.]\n",
            " [-1.  1.]\n",
            " [ 1.  2.]]\n",
            "[[-1. -1.  1.]\n",
            " [ 0.  1.  1.]]\n",
            "-0.8807970779778823 -0.8807970779844964\n",
            "0.8807970779778824 0.8807970779844964\n",
            "-0.8333712048003156 -0.8333712048225194\n",
            "0.8333712048003158 0.8333712048225194\n",
            "0.9282229511554491 0.9282229511464734\n",
            "-0.9282229511554491 -0.9282229511464734\n",
            "Gradient check passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG6H0-dPRrH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearSoftmaxClassifier():\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y, batch_size=5, learning_rate=1e-7, reg=1e-5,\n",
        "            epochs=1):\n",
        "        '''\n",
        "        Trains linear classifier\n",
        "        \n",
        "        Arguments:\n",
        "          X, np array (num_samples, num_features) - training data\n",
        "          y, np array of int (num_samples) - labels\n",
        "          batch_size, int - batch size to use\n",
        "          learning_rate, float - learning rate for gradient descent\n",
        "          reg, float - L2 regularization strength\n",
        "          epochs, int - number of epochs\n",
        "        '''\n",
        "\n",
        "        num_train = X.shape[0]\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = np.max(y)+1\n",
        "        if self.W is None:\n",
        "            self.W = 0.001 * np.random.randn(num_features, num_classes)\n",
        "\n",
        "        loss_history = []\n",
        "        for epoch in range(epochs):\n",
        "            shuffled_indices = np.arange(num_train)\n",
        "            np.random.shuffle(shuffled_indices)\n",
        "            sections = np.arange(batch_size, num_train, batch_size)\n",
        "            batches_indices = np.array_split(shuffled_indices, sections)\n",
        "            epoch_loss = []\n",
        "            for i in range(len(batches_indices)):\n",
        "                # X\n",
        "                batch = X[batches_indices[i]]\n",
        "                # Y\n",
        "                target = y[batches_indices[i]]\n",
        "                loss, dW = linear_softmax(batch, self.W, target)\n",
        "                # add l2 regularization\n",
        "                loss += reg * np.sum(np.square(self.W))\n",
        "                dW += 2 * reg * self.W\n",
        "                # weight update\n",
        "                self.W -= learning_rate * dW\n",
        "                \n",
        "                epoch_loss.append(loss)\n",
        "            epoch_loss = np.mean(epoch_loss)\n",
        "        \n",
        "            #end\n",
        "            print(\"Epoch %i, loss: %f\" % (epoch, epoch_loss))\n",
        "            loss_history.append(epoch_loss)\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Produces classifier predictions on the set\n",
        "       \n",
        "        Arguments:\n",
        "          X, np array (test_samples, num_features)\n",
        "\n",
        "        Returns:\n",
        "          y_pred, np.array of int (test_samples)\n",
        "        '''\n",
        "        y_pred = np.dot(X, self.W).argmax(axis=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AS5Y3rtRRrIH",
        "colab_type": "code",
        "colab": {},
        "outputId": "e30838e5-fcb9-4505-ce8d-3c7fda92f27b"
      },
      "source": [
        "# Let's check how it performs on validation set\n",
        "classifier = LinearSoftmaxClassifier()\n",
        "\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Now, let's train more and see if it performs better\n",
        "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-4, batch_size=300, reg=1e-6)\n",
        "pred = classifier.predict(val_X)\n",
        "accuracy = multiclass_accuracy(pred, val_y)\n",
        "print(\"Accuracy after training for 100 epochs: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.237\n",
            "Epoch 0, loss: 2.175519\n",
            "Epoch 1, loss: 2.171229\n",
            "Epoch 2, loss: 2.166625\n",
            "Epoch 3, loss: 2.163731\n",
            "Epoch 4, loss: 2.160160\n",
            "Epoch 5, loss: 2.157405\n",
            "Epoch 6, loss: 2.154834\n",
            "Epoch 7, loss: 2.151925\n",
            "Epoch 8, loss: 2.149470\n",
            "Epoch 9, loss: 2.147739\n",
            "Epoch 10, loss: 2.145332\n",
            "Epoch 11, loss: 2.144307\n",
            "Epoch 12, loss: 2.141439\n",
            "Epoch 13, loss: 2.139962\n",
            "Epoch 14, loss: 2.138375\n",
            "Epoch 15, loss: 2.137155\n",
            "Epoch 16, loss: 2.135265\n",
            "Epoch 17, loss: 2.133773\n",
            "Epoch 18, loss: 2.132158\n",
            "Epoch 19, loss: 2.131099\n",
            "Epoch 20, loss: 2.130079\n",
            "Epoch 21, loss: 2.128374\n",
            "Epoch 22, loss: 2.126958\n",
            "Epoch 23, loss: 2.126249\n",
            "Epoch 24, loss: 2.124995\n",
            "Epoch 25, loss: 2.124023\n",
            "Epoch 26, loss: 2.123141\n",
            "Epoch 27, loss: 2.121611\n",
            "Epoch 28, loss: 2.121093\n",
            "Epoch 29, loss: 2.119966\n",
            "Epoch 30, loss: 2.119203\n",
            "Epoch 31, loss: 2.117631\n",
            "Epoch 32, loss: 2.117739\n",
            "Epoch 33, loss: 2.117146\n",
            "Epoch 34, loss: 2.115582\n",
            "Epoch 35, loss: 2.115289\n",
            "Epoch 36, loss: 2.114254\n",
            "Epoch 37, loss: 2.114077\n",
            "Epoch 38, loss: 2.113000\n",
            "Epoch 39, loss: 2.111300\n",
            "Epoch 40, loss: 2.111241\n",
            "Epoch 41, loss: 2.110515\n",
            "Epoch 42, loss: 2.109600\n",
            "Epoch 43, loss: 2.109466\n",
            "Epoch 44, loss: 2.108021\n",
            "Epoch 45, loss: 2.107452\n",
            "Epoch 46, loss: 2.106418\n",
            "Epoch 47, loss: 2.106148\n",
            "Epoch 48, loss: 2.105901\n",
            "Epoch 49, loss: 2.104734\n",
            "Epoch 50, loss: 2.104780\n",
            "Epoch 51, loss: 2.103489\n",
            "Epoch 52, loss: 2.103376\n",
            "Epoch 53, loss: 2.102774\n",
            "Epoch 54, loss: 2.102039\n",
            "Epoch 55, loss: 2.101290\n",
            "Epoch 56, loss: 2.100383\n",
            "Epoch 57, loss: 2.100266\n",
            "Epoch 58, loss: 2.099262\n",
            "Epoch 59, loss: 2.099213\n",
            "Epoch 60, loss: 2.098215\n",
            "Epoch 61, loss: 2.098230\n",
            "Epoch 62, loss: 2.097720\n",
            "Epoch 63, loss: 2.097524\n",
            "Epoch 64, loss: 2.095859\n",
            "Epoch 65, loss: 2.095917\n",
            "Epoch 66, loss: 2.095286\n",
            "Epoch 67, loss: 2.094698\n",
            "Epoch 68, loss: 2.094664\n",
            "Epoch 69, loss: 2.093801\n",
            "Epoch 70, loss: 2.092904\n",
            "Epoch 71, loss: 2.093785\n",
            "Epoch 72, loss: 2.092854\n",
            "Epoch 73, loss: 2.092517\n",
            "Epoch 74, loss: 2.091384\n",
            "Epoch 75, loss: 2.091477\n",
            "Epoch 76, loss: 2.090494\n",
            "Epoch 77, loss: 2.090110\n",
            "Epoch 78, loss: 2.089683\n",
            "Epoch 79, loss: 2.089776\n",
            "Epoch 80, loss: 2.088677\n",
            "Epoch 81, loss: 2.088300\n",
            "Epoch 82, loss: 2.088126\n",
            "Epoch 83, loss: 2.087478\n",
            "Epoch 84, loss: 2.087337\n",
            "Epoch 85, loss: 2.086949\n",
            "Epoch 86, loss: 2.086663\n",
            "Epoch 87, loss: 2.086486\n",
            "Epoch 88, loss: 2.086091\n",
            "Epoch 89, loss: 2.085488\n",
            "Epoch 90, loss: 2.085412\n",
            "Epoch 91, loss: 2.084456\n",
            "Epoch 92, loss: 2.083321\n",
            "Epoch 93, loss: 2.083622\n",
            "Epoch 94, loss: 2.083465\n",
            "Epoch 95, loss: 2.082812\n",
            "Epoch 96, loss: 2.082482\n",
            "Epoch 97, loss: 2.081607\n",
            "Epoch 98, loss: 2.081781\n",
            "Epoch 99, loss: 2.081617\n",
            "Accuracy after training for 100 epochs:  0.245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "a_E1V-sORrIS",
        "colab_type": "code",
        "colab": {},
        "outputId": "a40ed383-7a4e-4ccc-bb67-971c11a5ac07"
      },
      "source": [
        "num_epochs = 200\n",
        "batch_size = 300\n",
        "\n",
        "learning_rates = [1e-4, 1e-5, 1e-7]\n",
        "reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6]\n",
        "\n",
        "best_classifier = None\n",
        "best_val_accuracy = 0\n",
        "best_history = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in reg_strengths:\n",
        "        classifier = LinearSoftmaxClassifier()\n",
        "        loss_history = classifier.fit(train_X, \n",
        "                                      train_y, \n",
        "                                      epochs=num_epochs, \n",
        "                                      learning_rate=lr, \n",
        "                                      batch_size=batch_size, \n",
        "                                      reg=reg)\n",
        "        \n",
        "        pred = classifier.predict(val_X)\n",
        "        accuracy = multiclass_accuracy(pred, val_y)\n",
        "        \n",
        "        if accuracy > best_val_accuracy:\n",
        "            best_classifier = classifier\n",
        "            best_val_accuracy = accuracy\n",
        "            best_history = loss_history\n",
        "        \n",
        "        print('current accuracy = ', accuracy)\n",
        "        print('----------------------------------')\n",
        "\n",
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss: 2.293854\n",
            "Epoch 1, loss: 2.272100\n",
            "Epoch 2, loss: 2.255290\n",
            "Epoch 3, loss: 2.239571\n",
            "Epoch 4, loss: 2.228289\n",
            "Epoch 5, loss: 2.217225\n",
            "Epoch 6, loss: 2.208166\n",
            "Epoch 7, loss: 2.200153\n",
            "Epoch 8, loss: 2.193511\n",
            "Epoch 9, loss: 2.187995\n",
            "Epoch 10, loss: 2.183011\n",
            "Epoch 11, loss: 2.178604\n",
            "Epoch 12, loss: 2.175043\n",
            "Epoch 13, loss: 2.170544\n",
            "Epoch 14, loss: 2.167460\n",
            "Epoch 15, loss: 2.165400\n",
            "Epoch 16, loss: 2.161914\n",
            "Epoch 17, loss: 2.158938\n",
            "Epoch 18, loss: 2.157094\n",
            "Epoch 19, loss: 2.154335\n",
            "Epoch 20, loss: 2.152889\n",
            "Epoch 21, loss: 2.150565\n",
            "Epoch 22, loss: 2.149226\n",
            "Epoch 23, loss: 2.147881\n",
            "Epoch 24, loss: 2.146073\n",
            "Epoch 25, loss: 2.144064\n",
            "Epoch 26, loss: 2.143192\n",
            "Epoch 27, loss: 2.141091\n",
            "Epoch 28, loss: 2.139878\n",
            "Epoch 29, loss: 2.138554\n",
            "Epoch 30, loss: 2.137349\n",
            "Epoch 31, loss: 2.135893\n",
            "Epoch 32, loss: 2.135411\n",
            "Epoch 33, loss: 2.134291\n",
            "Epoch 34, loss: 2.132688\n",
            "Epoch 35, loss: 2.131586\n",
            "Epoch 36, loss: 2.129657\n",
            "Epoch 37, loss: 2.129557\n",
            "Epoch 38, loss: 2.128449\n",
            "Epoch 39, loss: 2.128033\n",
            "Epoch 40, loss: 2.126717\n",
            "Epoch 41, loss: 2.125676\n",
            "Epoch 42, loss: 2.124789\n",
            "Epoch 43, loss: 2.124722\n",
            "Epoch 44, loss: 2.123514\n",
            "Epoch 45, loss: 2.122708\n",
            "Epoch 46, loss: 2.122086\n",
            "Epoch 47, loss: 2.121680\n",
            "Epoch 48, loss: 2.120045\n",
            "Epoch 49, loss: 2.120619\n",
            "Epoch 50, loss: 2.118931\n",
            "Epoch 51, loss: 2.118947\n",
            "Epoch 52, loss: 2.117754\n",
            "Epoch 53, loss: 2.116822\n",
            "Epoch 54, loss: 2.115698\n",
            "Epoch 55, loss: 2.116049\n",
            "Epoch 56, loss: 2.115240\n",
            "Epoch 57, loss: 2.114832\n",
            "Epoch 58, loss: 2.114399\n",
            "Epoch 59, loss: 2.113078\n",
            "Epoch 60, loss: 2.112995\n",
            "Epoch 61, loss: 2.112619\n",
            "Epoch 62, loss: 2.112528\n",
            "Epoch 63, loss: 2.112046\n",
            "Epoch 64, loss: 2.111187\n",
            "Epoch 65, loss: 2.110197\n",
            "Epoch 66, loss: 2.109838\n",
            "Epoch 67, loss: 2.109176\n",
            "Epoch 68, loss: 2.108900\n",
            "Epoch 69, loss: 2.108323\n",
            "Epoch 70, loss: 2.108098\n",
            "Epoch 71, loss: 2.107677\n",
            "Epoch 72, loss: 2.107415\n",
            "Epoch 73, loss: 2.106945\n",
            "Epoch 74, loss: 2.106091\n",
            "Epoch 75, loss: 2.105897\n",
            "Epoch 76, loss: 2.105382\n",
            "Epoch 77, loss: 2.105244\n",
            "Epoch 78, loss: 2.104395\n",
            "Epoch 79, loss: 2.104686\n",
            "Epoch 80, loss: 2.103756\n",
            "Epoch 81, loss: 2.103393\n",
            "Epoch 82, loss: 2.103562\n",
            "Epoch 83, loss: 2.102970\n",
            "Epoch 84, loss: 2.102237\n",
            "Epoch 85, loss: 2.101642\n",
            "Epoch 86, loss: 2.100847\n",
            "Epoch 87, loss: 2.101249\n",
            "Epoch 88, loss: 2.100710\n",
            "Epoch 89, loss: 2.100724\n",
            "Epoch 90, loss: 2.099963\n",
            "Epoch 91, loss: 2.099538\n",
            "Epoch 92, loss: 2.099752\n",
            "Epoch 93, loss: 2.099476\n",
            "Epoch 94, loss: 2.098357\n",
            "Epoch 95, loss: 2.098804\n",
            "Epoch 96, loss: 2.098550\n",
            "Epoch 97, loss: 2.097779\n",
            "Epoch 98, loss: 2.097784\n",
            "Epoch 99, loss: 2.097207\n",
            "Epoch 100, loss: 2.097253\n",
            "Epoch 101, loss: 2.096930\n",
            "Epoch 102, loss: 2.096854\n",
            "Epoch 103, loss: 2.096255\n",
            "Epoch 104, loss: 2.095744\n",
            "Epoch 105, loss: 2.095494\n",
            "Epoch 106, loss: 2.095605\n",
            "Epoch 107, loss: 2.094370\n",
            "Epoch 108, loss: 2.094547\n",
            "Epoch 109, loss: 2.094742\n",
            "Epoch 110, loss: 2.094213\n",
            "Epoch 111, loss: 2.093760\n",
            "Epoch 112, loss: 2.093501\n",
            "Epoch 113, loss: 2.093600\n",
            "Epoch 114, loss: 2.093307\n",
            "Epoch 115, loss: 2.092859\n",
            "Epoch 116, loss: 2.092414\n",
            "Epoch 117, loss: 2.092561\n",
            "Epoch 118, loss: 2.091815\n",
            "Epoch 119, loss: 2.091864\n",
            "Epoch 120, loss: 2.091299\n",
            "Epoch 121, loss: 2.091289\n",
            "Epoch 122, loss: 2.091774\n",
            "Epoch 123, loss: 2.090693\n",
            "Epoch 124, loss: 2.091104\n",
            "Epoch 125, loss: 2.090025\n",
            "Epoch 126, loss: 2.089997\n",
            "Epoch 127, loss: 2.090534\n",
            "Epoch 128, loss: 2.090008\n",
            "Epoch 129, loss: 2.089269\n",
            "Epoch 130, loss: 2.088262\n",
            "Epoch 131, loss: 2.089129\n",
            "Epoch 132, loss: 2.089247\n",
            "Epoch 133, loss: 2.088436\n",
            "Epoch 134, loss: 2.089065\n",
            "Epoch 135, loss: 2.087991\n",
            "Epoch 136, loss: 2.087628\n",
            "Epoch 137, loss: 2.087991\n",
            "Epoch 138, loss: 2.088145\n",
            "Epoch 139, loss: 2.087389\n",
            "Epoch 140, loss: 2.087415\n",
            "Epoch 141, loss: 2.087241\n",
            "Epoch 142, loss: 2.087191\n",
            "Epoch 143, loss: 2.086937\n",
            "Epoch 144, loss: 2.086258\n",
            "Epoch 145, loss: 2.086562\n",
            "Epoch 146, loss: 2.086444\n",
            "Epoch 147, loss: 2.086258\n",
            "Epoch 148, loss: 2.086322\n",
            "Epoch 149, loss: 2.086122\n",
            "Epoch 150, loss: 2.086106\n",
            "Epoch 151, loss: 2.084843\n",
            "Epoch 152, loss: 2.085342\n",
            "Epoch 153, loss: 2.085160\n",
            "Epoch 154, loss: 2.084417\n",
            "Epoch 155, loss: 2.084595\n",
            "Epoch 156, loss: 2.084777\n",
            "Epoch 157, loss: 2.084227\n",
            "Epoch 158, loss: 2.083884\n",
            "Epoch 159, loss: 2.083492\n",
            "Epoch 160, loss: 2.083643\n",
            "Epoch 161, loss: 2.083277\n",
            "Epoch 162, loss: 2.083393\n",
            "Epoch 163, loss: 2.083075\n",
            "Epoch 164, loss: 2.083409\n",
            "Epoch 165, loss: 2.083370\n",
            "Epoch 166, loss: 2.081995\n",
            "Epoch 167, loss: 2.081948\n",
            "Epoch 168, loss: 2.083306\n",
            "Epoch 169, loss: 2.082799\n",
            "Epoch 170, loss: 2.082140\n",
            "Epoch 171, loss: 2.082520\n",
            "Epoch 172, loss: 2.081820\n",
            "Epoch 173, loss: 2.082142\n",
            "Epoch 174, loss: 2.081775\n",
            "Epoch 175, loss: 2.080941\n",
            "Epoch 176, loss: 2.081431\n",
            "Epoch 177, loss: 2.080828\n",
            "Epoch 178, loss: 2.080870\n",
            "Epoch 179, loss: 2.080836\n",
            "Epoch 180, loss: 2.080778\n",
            "Epoch 181, loss: 2.080258\n",
            "Epoch 182, loss: 2.080679\n",
            "Epoch 183, loss: 2.080191\n",
            "Epoch 184, loss: 2.080595\n",
            "Epoch 185, loss: 2.080795\n",
            "Epoch 186, loss: 2.080064\n",
            "Epoch 187, loss: 2.079399\n",
            "Epoch 188, loss: 2.079399\n",
            "Epoch 189, loss: 2.079109\n",
            "Epoch 190, loss: 2.079141\n",
            "Epoch 191, loss: 2.079148\n",
            "Epoch 192, loss: 2.078718\n",
            "Epoch 193, loss: 2.079577\n",
            "Epoch 194, loss: 2.078962\n",
            "Epoch 195, loss: 2.079099\n",
            "Epoch 196, loss: 2.079206\n",
            "Epoch 197, loss: 2.079056\n",
            "Epoch 198, loss: 2.079249\n",
            "Epoch 199, loss: 2.078126\n",
            "current accuracy =  0.248\n",
            "----------------------------------\n",
            "Epoch 0, loss: 2.294283\n",
            "Epoch 1, loss: 2.272326\n",
            "Epoch 2, loss: 2.254503\n",
            "Epoch 3, loss: 2.239367\n",
            "Epoch 4, loss: 2.227191\n",
            "Epoch 5, loss: 2.216678\n",
            "Epoch 6, loss: 2.208010\n",
            "Epoch 7, loss: 2.199783\n",
            "Epoch 8, loss: 2.193383\n",
            "Epoch 9, loss: 2.187056\n",
            "Epoch 10, loss: 2.181815\n",
            "Epoch 11, loss: 2.177089\n",
            "Epoch 12, loss: 2.172577\n",
            "Epoch 13, loss: 2.170143\n",
            "Epoch 14, loss: 2.166077\n",
            "Epoch 15, loss: 2.163179\n",
            "Epoch 16, loss: 2.159939\n",
            "Epoch 17, loss: 2.157490\n",
            "Epoch 18, loss: 2.154883\n",
            "Epoch 19, loss: 2.152671\n",
            "Epoch 20, loss: 2.150769\n",
            "Epoch 21, loss: 2.148686\n",
            "Epoch 22, loss: 2.146869\n",
            "Epoch 23, loss: 2.145217\n",
            "Epoch 24, loss: 2.143455\n",
            "Epoch 25, loss: 2.141716\n",
            "Epoch 26, loss: 2.140129\n",
            "Epoch 27, loss: 2.138843\n",
            "Epoch 28, loss: 2.136908\n",
            "Epoch 29, loss: 2.134770\n",
            "Epoch 30, loss: 2.134811\n",
            "Epoch 31, loss: 2.132661\n",
            "Epoch 32, loss: 2.131520\n",
            "Epoch 33, loss: 2.130403\n",
            "Epoch 34, loss: 2.129067\n",
            "Epoch 35, loss: 2.128339\n",
            "Epoch 36, loss: 2.126566\n",
            "Epoch 37, loss: 2.125739\n",
            "Epoch 38, loss: 2.124557\n",
            "Epoch 39, loss: 2.124412\n",
            "Epoch 40, loss: 2.123557\n",
            "Epoch 41, loss: 2.121918\n",
            "Epoch 42, loss: 2.120930\n",
            "Epoch 43, loss: 2.120315\n",
            "Epoch 44, loss: 2.118982\n",
            "Epoch 45, loss: 2.118111\n",
            "Epoch 46, loss: 2.116788\n",
            "Epoch 47, loss: 2.116373\n",
            "Epoch 48, loss: 2.115952\n",
            "Epoch 49, loss: 2.115035\n",
            "Epoch 50, loss: 2.114055\n",
            "Epoch 51, loss: 2.113146\n",
            "Epoch 52, loss: 2.112512\n",
            "Epoch 53, loss: 2.111646\n",
            "Epoch 54, loss: 2.111191\n",
            "Epoch 55, loss: 2.109969\n",
            "Epoch 56, loss: 2.109517\n",
            "Epoch 57, loss: 2.109357\n",
            "Epoch 58, loss: 2.107966\n",
            "Epoch 59, loss: 2.107887\n",
            "Epoch 60, loss: 2.107987\n",
            "Epoch 61, loss: 2.106540\n",
            "Epoch 62, loss: 2.105502\n",
            "Epoch 63, loss: 2.104365\n",
            "Epoch 64, loss: 2.104578\n",
            "Epoch 65, loss: 2.103927\n",
            "Epoch 66, loss: 2.104075\n",
            "Epoch 67, loss: 2.102308\n",
            "Epoch 68, loss: 2.102485\n",
            "Epoch 69, loss: 2.101328\n",
            "Epoch 70, loss: 2.101624\n",
            "Epoch 71, loss: 2.099864\n",
            "Epoch 72, loss: 2.100353\n",
            "Epoch 73, loss: 2.099589\n",
            "Epoch 74, loss: 2.099444\n",
            "Epoch 75, loss: 2.098441\n",
            "Epoch 76, loss: 2.098108\n",
            "Epoch 77, loss: 2.096575\n",
            "Epoch 78, loss: 2.097470\n",
            "Epoch 79, loss: 2.096766\n",
            "Epoch 80, loss: 2.095473\n",
            "Epoch 81, loss: 2.095449\n",
            "Epoch 82, loss: 2.094733\n",
            "Epoch 83, loss: 2.095084\n",
            "Epoch 84, loss: 2.093640\n",
            "Epoch 85, loss: 2.093451\n",
            "Epoch 86, loss: 2.093481\n",
            "Epoch 87, loss: 2.092515\n",
            "Epoch 88, loss: 2.091915\n",
            "Epoch 89, loss: 2.091518\n",
            "Epoch 90, loss: 2.090900\n",
            "Epoch 91, loss: 2.091056\n",
            "Epoch 92, loss: 2.090587\n",
            "Epoch 93, loss: 2.090640\n",
            "Epoch 94, loss: 2.090107\n",
            "Epoch 95, loss: 2.089477\n",
            "Epoch 96, loss: 2.088845\n",
            "Epoch 97, loss: 2.089092\n",
            "Epoch 98, loss: 2.088279\n",
            "Epoch 99, loss: 2.087325\n",
            "Epoch 100, loss: 2.087624\n",
            "Epoch 101, loss: 2.086569\n",
            "Epoch 102, loss: 2.086408\n",
            "Epoch 103, loss: 2.086889\n",
            "Epoch 104, loss: 2.085868\n",
            "Epoch 105, loss: 2.085928\n",
            "Epoch 106, loss: 2.084935\n",
            "Epoch 107, loss: 2.085226\n",
            "Epoch 108, loss: 2.084252\n",
            "Epoch 109, loss: 2.084448\n",
            "Epoch 110, loss: 2.084197\n",
            "Epoch 111, loss: 2.083321\n",
            "Epoch 112, loss: 2.082725\n",
            "Epoch 113, loss: 2.082425\n",
            "Epoch 114, loss: 2.082041\n",
            "Epoch 115, loss: 2.082160\n",
            "Epoch 116, loss: 2.081783\n",
            "Epoch 117, loss: 2.082007\n",
            "Epoch 118, loss: 2.081254\n",
            "Epoch 119, loss: 2.080449\n",
            "Epoch 120, loss: 2.080582\n",
            "Epoch 121, loss: 2.080012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 122, loss: 2.079671\n",
            "Epoch 123, loss: 2.079260\n",
            "Epoch 124, loss: 2.078618\n",
            "Epoch 125, loss: 2.079209\n",
            "Epoch 126, loss: 2.078618\n",
            "Epoch 127, loss: 2.077897\n",
            "Epoch 128, loss: 2.077843\n",
            "Epoch 129, loss: 2.077348\n",
            "Epoch 130, loss: 2.077016\n",
            "Epoch 131, loss: 2.077017\n",
            "Epoch 132, loss: 2.076006\n",
            "Epoch 133, loss: 2.075771\n",
            "Epoch 134, loss: 2.075456\n",
            "Epoch 135, loss: 2.075729\n",
            "Epoch 136, loss: 2.074661\n",
            "Epoch 137, loss: 2.074817\n",
            "Epoch 138, loss: 2.074619\n",
            "Epoch 139, loss: 2.074422\n",
            "Epoch 140, loss: 2.073325\n",
            "Epoch 141, loss: 2.073955\n",
            "Epoch 142, loss: 2.073770\n",
            "Epoch 143, loss: 2.072536\n",
            "Epoch 144, loss: 2.073247\n",
            "Epoch 145, loss: 2.072579\n",
            "Epoch 146, loss: 2.072426\n",
            "Epoch 147, loss: 2.072740\n",
            "Epoch 148, loss: 2.071488\n",
            "Epoch 149, loss: 2.071172\n",
            "Epoch 150, loss: 2.071733\n",
            "Epoch 151, loss: 2.070808\n",
            "Epoch 152, loss: 2.070386\n",
            "Epoch 153, loss: 2.070359\n",
            "Epoch 154, loss: 2.070469\n",
            "Epoch 155, loss: 2.070027\n",
            "Epoch 156, loss: 2.069809\n",
            "Epoch 157, loss: 2.069105\n",
            "Epoch 158, loss: 2.069214\n",
            "Epoch 159, loss: 2.068495\n",
            "Epoch 160, loss: 2.068702\n",
            "Epoch 161, loss: 2.068848\n",
            "Epoch 162, loss: 2.068607\n",
            "Epoch 163, loss: 2.067765\n",
            "Epoch 164, loss: 2.067979\n",
            "Epoch 165, loss: 2.067416\n",
            "Epoch 166, loss: 2.066804\n",
            "Epoch 167, loss: 2.067204\n",
            "Epoch 168, loss: 2.066975\n",
            "Epoch 169, loss: 2.066164\n",
            "Epoch 170, loss: 2.066055\n",
            "Epoch 171, loss: 2.065560\n",
            "Epoch 172, loss: 2.065269\n",
            "Epoch 173, loss: 2.065795\n",
            "Epoch 174, loss: 2.065243\n",
            "Epoch 175, loss: 2.064916\n",
            "Epoch 176, loss: 2.064737\n",
            "Epoch 177, loss: 2.064779\n",
            "Epoch 178, loss: 2.064406\n",
            "Epoch 179, loss: 2.063594\n",
            "Epoch 180, loss: 2.064586\n",
            "Epoch 181, loss: 2.063371\n",
            "Epoch 182, loss: 2.062872\n",
            "Epoch 183, loss: 2.063359\n",
            "Epoch 184, loss: 2.062723\n",
            "Epoch 185, loss: 2.062048\n",
            "Epoch 186, loss: 2.062805\n",
            "Epoch 187, loss: 2.062634\n",
            "Epoch 188, loss: 2.061877\n",
            "Epoch 189, loss: 2.061592\n",
            "Epoch 190, loss: 2.061561\n",
            "Epoch 191, loss: 2.061244\n",
            "Epoch 192, loss: 2.060858\n",
            "Epoch 193, loss: 2.060601\n",
            "Epoch 194, loss: 2.061104\n",
            "Epoch 195, loss: 2.060366\n",
            "Epoch 196, loss: 2.059873\n",
            "Epoch 197, loss: 2.059660\n",
            "Epoch 198, loss: 2.060265\n",
            "Epoch 199, loss: 2.059761\n",
            "current accuracy =  0.25\n",
            "----------------------------------\n",
            "Epoch 0, loss: 2.293728\n",
            "Epoch 1, loss: 2.272799\n",
            "Epoch 2, loss: 2.254604\n",
            "Epoch 3, loss: 2.239647\n",
            "Epoch 4, loss: 2.227296\n",
            "Epoch 5, loss: 2.216030\n",
            "Epoch 6, loss: 2.207396\n",
            "Epoch 7, loss: 2.200237\n",
            "Epoch 8, loss: 2.192954\n",
            "Epoch 9, loss: 2.187344\n",
            "Epoch 10, loss: 2.181314\n",
            "Epoch 11, loss: 2.177455\n",
            "Epoch 12, loss: 2.173555\n",
            "Epoch 13, loss: 2.169602\n",
            "Epoch 14, loss: 2.166128\n",
            "Epoch 15, loss: 2.162884\n",
            "Epoch 16, loss: 2.159848\n",
            "Epoch 17, loss: 2.157516\n",
            "Epoch 18, loss: 2.155094\n",
            "Epoch 19, loss: 2.153048\n",
            "Epoch 20, loss: 2.150417\n",
            "Epoch 21, loss: 2.148419\n",
            "Epoch 22, loss: 2.147173\n",
            "Epoch 23, loss: 2.144773\n",
            "Epoch 24, loss: 2.143002\n",
            "Epoch 25, loss: 2.140963\n",
            "Epoch 26, loss: 2.140073\n",
            "Epoch 27, loss: 2.137920\n",
            "Epoch 28, loss: 2.136587\n",
            "Epoch 29, loss: 2.134628\n",
            "Epoch 30, loss: 2.134204\n",
            "Epoch 31, loss: 2.132425\n",
            "Epoch 32, loss: 2.131678\n",
            "Epoch 33, loss: 2.129813\n",
            "Epoch 34, loss: 2.128705\n",
            "Epoch 35, loss: 2.127651\n",
            "Epoch 36, loss: 2.126697\n",
            "Epoch 37, loss: 2.125278\n",
            "Epoch 38, loss: 2.124443\n",
            "Epoch 39, loss: 2.122856\n",
            "Epoch 40, loss: 2.122020\n",
            "Epoch 41, loss: 2.121550\n",
            "Epoch 42, loss: 2.119885\n",
            "Epoch 43, loss: 2.118841\n",
            "Epoch 44, loss: 2.118774\n",
            "Epoch 45, loss: 2.118401\n",
            "Epoch 46, loss: 2.116761\n",
            "Epoch 47, loss: 2.116098\n",
            "Epoch 48, loss: 2.115174\n",
            "Epoch 49, loss: 2.113936\n",
            "Epoch 50, loss: 2.114092\n",
            "Epoch 51, loss: 2.113400\n",
            "Epoch 52, loss: 2.112230\n",
            "Epoch 53, loss: 2.111778\n",
            "Epoch 54, loss: 2.110930\n",
            "Epoch 55, loss: 2.109857\n",
            "Epoch 56, loss: 2.109215\n",
            "Epoch 57, loss: 2.108175\n",
            "Epoch 58, loss: 2.108050\n",
            "Epoch 59, loss: 2.106560\n",
            "Epoch 60, loss: 2.106558\n",
            "Epoch 61, loss: 2.105428\n",
            "Epoch 62, loss: 2.105206\n",
            "Epoch 63, loss: 2.104648\n",
            "Epoch 64, loss: 2.103512\n",
            "Epoch 65, loss: 2.103411\n",
            "Epoch 66, loss: 2.102246\n",
            "Epoch 67, loss: 2.102602\n",
            "Epoch 68, loss: 2.102013\n",
            "Epoch 69, loss: 2.100958\n",
            "Epoch 70, loss: 2.100074\n",
            "Epoch 71, loss: 2.100005\n",
            "Epoch 72, loss: 2.099044\n",
            "Epoch 73, loss: 2.098613\n",
            "Epoch 74, loss: 2.098240\n",
            "Epoch 75, loss: 2.097943\n",
            "Epoch 76, loss: 2.097235\n",
            "Epoch 77, loss: 2.096459\n",
            "Epoch 78, loss: 2.097094\n",
            "Epoch 79, loss: 2.095609\n",
            "Epoch 80, loss: 2.095402\n",
            "Epoch 81, loss: 2.094607\n",
            "Epoch 82, loss: 2.094123\n",
            "Epoch 83, loss: 2.093962\n",
            "Epoch 84, loss: 2.092684\n",
            "Epoch 85, loss: 2.092836\n",
            "Epoch 86, loss: 2.091776\n",
            "Epoch 87, loss: 2.092616\n",
            "Epoch 88, loss: 2.091176\n",
            "Epoch 89, loss: 2.091577\n",
            "Epoch 90, loss: 2.090967\n",
            "Epoch 91, loss: 2.089947\n",
            "Epoch 92, loss: 2.090101\n",
            "Epoch 93, loss: 2.088842\n",
            "Epoch 94, loss: 2.089045\n",
            "Epoch 95, loss: 2.088331\n",
            "Epoch 96, loss: 2.087607\n",
            "Epoch 97, loss: 2.087745\n",
            "Epoch 98, loss: 2.087355\n",
            "Epoch 99, loss: 2.086511\n",
            "Epoch 100, loss: 2.086818\n",
            "Epoch 101, loss: 2.086446\n",
            "Epoch 102, loss: 2.085314\n",
            "Epoch 103, loss: 2.084716\n",
            "Epoch 104, loss: 2.084091\n",
            "Epoch 105, loss: 2.084759\n",
            "Epoch 106, loss: 2.084757\n",
            "Epoch 107, loss: 2.084136\n",
            "Epoch 108, loss: 2.083378\n",
            "Epoch 109, loss: 2.083495\n",
            "Epoch 110, loss: 2.082607\n",
            "Epoch 111, loss: 2.082327\n",
            "Epoch 112, loss: 2.081891\n",
            "Epoch 113, loss: 2.081782\n",
            "Epoch 114, loss: 2.081456\n",
            "Epoch 115, loss: 2.081136\n",
            "Epoch 116, loss: 2.080130\n",
            "Epoch 117, loss: 2.080083\n",
            "Epoch 118, loss: 2.079950\n",
            "Epoch 119, loss: 2.079456\n",
            "Epoch 120, loss: 2.078804\n",
            "Epoch 121, loss: 2.078796\n",
            "Epoch 122, loss: 2.078511\n",
            "Epoch 123, loss: 2.077881\n",
            "Epoch 124, loss: 2.078122\n",
            "Epoch 125, loss: 2.077544\n",
            "Epoch 126, loss: 2.076827\n",
            "Epoch 127, loss: 2.077100\n",
            "Epoch 128, loss: 2.077157\n",
            "Epoch 129, loss: 2.076329\n",
            "Epoch 130, loss: 2.075245\n",
            "Epoch 131, loss: 2.075812\n",
            "Epoch 132, loss: 2.075444\n",
            "Epoch 133, loss: 2.074807\n",
            "Epoch 134, loss: 2.074738\n",
            "Epoch 135, loss: 2.074585\n",
            "Epoch 136, loss: 2.073677\n",
            "Epoch 137, loss: 2.073439\n",
            "Epoch 138, loss: 2.072645\n",
            "Epoch 139, loss: 2.073376\n",
            "Epoch 140, loss: 2.072721\n",
            "Epoch 141, loss: 2.072547\n",
            "Epoch 142, loss: 2.072686\n",
            "Epoch 143, loss: 2.071986\n",
            "Epoch 144, loss: 2.071853\n",
            "Epoch 145, loss: 2.070567\n",
            "Epoch 146, loss: 2.071576\n",
            "Epoch 147, loss: 2.070661\n",
            "Epoch 148, loss: 2.069883\n",
            "Epoch 149, loss: 2.070651\n",
            "Epoch 150, loss: 2.069946\n",
            "Epoch 151, loss: 2.069483\n",
            "Epoch 152, loss: 2.069254\n",
            "Epoch 153, loss: 2.068780\n",
            "Epoch 154, loss: 2.069412\n",
            "Epoch 155, loss: 2.068536\n",
            "Epoch 156, loss: 2.068589\n",
            "Epoch 157, loss: 2.068275\n",
            "Epoch 158, loss: 2.067966\n",
            "Epoch 159, loss: 2.067658\n",
            "Epoch 160, loss: 2.067487\n",
            "Epoch 161, loss: 2.066769\n",
            "Epoch 162, loss: 2.065938\n",
            "Epoch 163, loss: 2.066421\n",
            "Epoch 164, loss: 2.066309\n",
            "Epoch 165, loss: 2.065423\n",
            "Epoch 166, loss: 2.065704\n",
            "Epoch 167, loss: 2.065065\n",
            "Epoch 168, loss: 2.065063\n",
            "Epoch 169, loss: 2.064882\n",
            "Epoch 170, loss: 2.064856\n",
            "Epoch 171, loss: 2.064015\n",
            "Epoch 172, loss: 2.064283\n",
            "Epoch 173, loss: 2.064047\n",
            "Epoch 174, loss: 2.063903\n",
            "Epoch 175, loss: 2.063582\n",
            "Epoch 176, loss: 2.062758\n",
            "Epoch 177, loss: 2.062292\n",
            "Epoch 178, loss: 2.062906\n",
            "Epoch 179, loss: 2.062196\n",
            "Epoch 180, loss: 2.061888\n",
            "Epoch 181, loss: 2.061485\n",
            "Epoch 182, loss: 2.062082\n",
            "Epoch 183, loss: 2.060760\n",
            "Epoch 184, loss: 2.062023\n",
            "Epoch 185, loss: 2.060686\n",
            "Epoch 186, loss: 2.060828\n",
            "Epoch 187, loss: 2.060654\n",
            "Epoch 188, loss: 2.060310\n",
            "Epoch 189, loss: 2.059727\n",
            "Epoch 190, loss: 2.059866\n",
            "Epoch 191, loss: 2.059571\n",
            "Epoch 192, loss: 2.058718\n",
            "Epoch 193, loss: 2.059116\n",
            "Epoch 194, loss: 2.059222\n",
            "Epoch 195, loss: 2.058077\n",
            "Epoch 196, loss: 2.058641\n",
            "Epoch 197, loss: 2.057990\n",
            "Epoch 198, loss: 2.057761\n",
            "Epoch 199, loss: 2.057831\n",
            "current accuracy =  0.251\n",
            "----------------------------------\n",
            "Epoch 0, loss: 2.293926\n",
            "Epoch 1, loss: 2.272295\n",
            "Epoch 2, loss: 2.254662\n",
            "Epoch 3, loss: 2.239525\n",
            "Epoch 4, loss: 2.227636\n",
            "Epoch 5, loss: 2.216100\n",
            "Epoch 6, loss: 2.206896\n",
            "Epoch 7, loss: 2.199254\n",
            "Epoch 8, loss: 2.192713\n",
            "Epoch 9, loss: 2.186736\n",
            "Epoch 10, loss: 2.182346\n",
            "Epoch 11, loss: 2.177292\n",
            "Epoch 12, loss: 2.173132\n",
            "Epoch 13, loss: 2.169155\n",
            "Epoch 14, loss: 2.166072\n",
            "Epoch 15, loss: 2.162607\n",
            "Epoch 16, loss: 2.160332\n",
            "Epoch 17, loss: 2.158088\n",
            "Epoch 18, loss: 2.154768\n",
            "Epoch 19, loss: 2.152481\n",
            "Epoch 20, loss: 2.150835\n",
            "Epoch 21, loss: 2.148651\n",
            "Epoch 22, loss: 2.146658\n",
            "Epoch 23, loss: 2.144465\n",
            "Epoch 24, loss: 2.143415\n",
            "Epoch 25, loss: 2.141122\n",
            "Epoch 26, loss: 2.139322\n",
            "Epoch 27, loss: 2.137818\n",
            "Epoch 28, loss: 2.136928\n",
            "Epoch 29, loss: 2.135008\n",
            "Epoch 30, loss: 2.134023\n",
            "Epoch 31, loss: 2.132808\n",
            "Epoch 32, loss: 2.131089\n",
            "Epoch 33, loss: 2.130154\n",
            "Epoch 34, loss: 2.129026\n",
            "Epoch 35, loss: 2.127486\n",
            "Epoch 36, loss: 2.126155\n",
            "Epoch 37, loss: 2.125790\n",
            "Epoch 38, loss: 2.124839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39, loss: 2.123329\n",
            "Epoch 40, loss: 2.122492\n",
            "Epoch 41, loss: 2.121585\n",
            "Epoch 42, loss: 2.120429\n",
            "Epoch 43, loss: 2.119983\n",
            "Epoch 44, loss: 2.118622\n",
            "Epoch 45, loss: 2.118080\n",
            "Epoch 46, loss: 2.116547\n",
            "Epoch 47, loss: 2.115850\n",
            "Epoch 48, loss: 2.115068\n",
            "Epoch 49, loss: 2.114527\n",
            "Epoch 50, loss: 2.113950\n",
            "Epoch 51, loss: 2.112290\n",
            "Epoch 52, loss: 2.112406\n",
            "Epoch 53, loss: 2.111567\n",
            "Epoch 54, loss: 2.111260\n",
            "Epoch 55, loss: 2.110009\n",
            "Epoch 56, loss: 2.109131\n",
            "Epoch 57, loss: 2.108365\n",
            "Epoch 58, loss: 2.107406\n",
            "Epoch 59, loss: 2.107240\n",
            "Epoch 60, loss: 2.106511\n",
            "Epoch 61, loss: 2.105605\n",
            "Epoch 62, loss: 2.105160\n",
            "Epoch 63, loss: 2.104404\n",
            "Epoch 64, loss: 2.103995\n",
            "Epoch 65, loss: 2.103418\n",
            "Epoch 66, loss: 2.103171\n",
            "Epoch 67, loss: 2.101981\n",
            "Epoch 68, loss: 2.101668\n",
            "Epoch 69, loss: 2.100600\n",
            "Epoch 70, loss: 2.100658\n",
            "Epoch 71, loss: 2.099856\n",
            "Epoch 72, loss: 2.099580\n",
            "Epoch 73, loss: 2.097831\n",
            "Epoch 74, loss: 2.097818\n",
            "Epoch 75, loss: 2.097955\n",
            "Epoch 76, loss: 2.096912\n",
            "Epoch 77, loss: 2.096175\n",
            "Epoch 78, loss: 2.095771\n",
            "Epoch 79, loss: 2.096298\n",
            "Epoch 80, loss: 2.095123\n",
            "Epoch 81, loss: 2.094801\n",
            "Epoch 82, loss: 2.094324\n",
            "Epoch 83, loss: 2.094241\n",
            "Epoch 84, loss: 2.093118\n",
            "Epoch 85, loss: 2.092469\n",
            "Epoch 86, loss: 2.092599\n",
            "Epoch 87, loss: 2.091488\n",
            "Epoch 88, loss: 2.091111\n",
            "Epoch 89, loss: 2.090782\n",
            "Epoch 90, loss: 2.090481\n",
            "Epoch 91, loss: 2.090138\n",
            "Epoch 92, loss: 2.090109\n",
            "Epoch 93, loss: 2.089519\n",
            "Epoch 94, loss: 2.089042\n",
            "Epoch 95, loss: 2.088797\n",
            "Epoch 96, loss: 2.088252\n",
            "Epoch 97, loss: 2.087974\n",
            "Epoch 98, loss: 2.087487\n",
            "Epoch 99, loss: 2.086477\n",
            "Epoch 100, loss: 2.086586\n",
            "Epoch 101, loss: 2.086073\n",
            "Epoch 102, loss: 2.085399\n",
            "Epoch 103, loss: 2.085296\n",
            "Epoch 104, loss: 2.085488\n",
            "Epoch 105, loss: 2.084452\n",
            "Epoch 106, loss: 2.084566\n",
            "Epoch 107, loss: 2.084329\n",
            "Epoch 108, loss: 2.083101\n",
            "Epoch 109, loss: 2.082237\n",
            "Epoch 110, loss: 2.082683\n",
            "Epoch 111, loss: 2.081997\n",
            "Epoch 112, loss: 2.081748\n",
            "Epoch 113, loss: 2.081568\n",
            "Epoch 114, loss: 2.081265\n",
            "Epoch 115, loss: 2.080138\n",
            "Epoch 116, loss: 2.080922\n",
            "Epoch 117, loss: 2.080844\n",
            "Epoch 118, loss: 2.079833\n",
            "Epoch 119, loss: 2.079307\n",
            "Epoch 120, loss: 2.080071\n",
            "Epoch 121, loss: 2.078485\n",
            "Epoch 122, loss: 2.078555\n",
            "Epoch 123, loss: 2.077710\n",
            "Epoch 124, loss: 2.077855\n",
            "Epoch 125, loss: 2.077087\n",
            "Epoch 126, loss: 2.077457\n",
            "Epoch 127, loss: 2.076762\n",
            "Epoch 128, loss: 2.076747\n",
            "Epoch 129, loss: 2.076271\n",
            "Epoch 130, loss: 2.075739\n",
            "Epoch 131, loss: 2.074634\n",
            "Epoch 132, loss: 2.075183\n",
            "Epoch 133, loss: 2.074820\n",
            "Epoch 134, loss: 2.074462\n",
            "Epoch 135, loss: 2.074259\n",
            "Epoch 136, loss: 2.073647\n",
            "Epoch 137, loss: 2.073276\n",
            "Epoch 138, loss: 2.073313\n",
            "Epoch 139, loss: 2.073464\n",
            "Epoch 140, loss: 2.072739\n",
            "Epoch 141, loss: 2.072782\n",
            "Epoch 142, loss: 2.072103\n",
            "Epoch 143, loss: 2.072034\n",
            "Epoch 144, loss: 2.071381\n",
            "Epoch 145, loss: 2.070866\n",
            "Epoch 146, loss: 2.070805\n",
            "Epoch 147, loss: 2.070487\n",
            "Epoch 148, loss: 2.069918\n",
            "Epoch 149, loss: 2.069621\n",
            "Epoch 150, loss: 2.069684\n",
            "Epoch 151, loss: 2.069304\n",
            "Epoch 152, loss: 2.069046\n",
            "Epoch 153, loss: 2.069134\n",
            "Epoch 154, loss: 2.068797\n",
            "Epoch 155, loss: 2.068761\n",
            "Epoch 156, loss: 2.068079\n",
            "Epoch 157, loss: 2.067586\n",
            "Epoch 158, loss: 2.067739\n",
            "Epoch 159, loss: 2.067309\n",
            "Epoch 160, loss: 2.066774\n",
            "Epoch 161, loss: 2.067067\n",
            "Epoch 162, loss: 2.066787\n",
            "Epoch 163, loss: 2.066341\n",
            "Epoch 164, loss: 2.065764\n",
            "Epoch 165, loss: 2.066493\n",
            "Epoch 166, loss: 2.066097\n",
            "Epoch 167, loss: 2.064849\n",
            "Epoch 168, loss: 2.065304\n",
            "Epoch 169, loss: 2.065363\n",
            "Epoch 170, loss: 2.064827\n",
            "Epoch 171, loss: 2.064339\n",
            "Epoch 172, loss: 2.064064\n",
            "Epoch 173, loss: 2.063850\n",
            "Epoch 174, loss: 2.063283\n",
            "Epoch 175, loss: 2.062908\n",
            "Epoch 176, loss: 2.063216\n",
            "Epoch 177, loss: 2.062879\n",
            "Epoch 178, loss: 2.062671\n",
            "Epoch 179, loss: 2.062324\n",
            "Epoch 180, loss: 2.062092\n",
            "Epoch 181, loss: 2.061788\n",
            "Epoch 182, loss: 2.061618\n",
            "Epoch 183, loss: 2.060679\n",
            "Epoch 184, loss: 2.060906\n",
            "Epoch 185, loss: 2.061371\n",
            "Epoch 186, loss: 2.060420\n",
            "Epoch 187, loss: 2.060321\n",
            "Epoch 188, loss: 2.060108\n",
            "Epoch 189, loss: 2.059753\n",
            "Epoch 190, loss: 2.059638\n",
            "Epoch 191, loss: 2.059556\n",
            "Epoch 192, loss: 2.059133\n",
            "Epoch 193, loss: 2.058798\n",
            "Epoch 194, loss: 2.058871\n",
            "Epoch 195, loss: 2.058643\n",
            "Epoch 196, loss: 2.058139\n",
            "Epoch 197, loss: 2.058085\n",
            "Epoch 198, loss: 2.057965\n",
            "Epoch 199, loss: 2.057973\n",
            "current accuracy =  0.248\n",
            "----------------------------------\n",
            "Epoch 0, loss: 2.301548\n",
            "Epoch 1, loss: 2.298720\n",
            "Epoch 2, loss: 2.296065\n",
            "Epoch 3, loss: 2.293417\n",
            "Epoch 4, loss: 2.290985\n",
            "Epoch 5, loss: 2.288509\n",
            "Epoch 6, loss: 2.286137\n",
            "Epoch 7, loss: 2.283828\n",
            "Epoch 8, loss: 2.281558\n",
            "Epoch 9, loss: 2.279352\n",
            "Epoch 10, loss: 2.277200\n",
            "Epoch 11, loss: 2.275102\n",
            "Epoch 12, loss: 2.273002\n",
            "Epoch 13, loss: 2.271005\n",
            "Epoch 14, loss: 2.269092\n",
            "Epoch 15, loss: 2.267100\n",
            "Epoch 16, loss: 2.265181\n",
            "Epoch 17, loss: 2.263316\n",
            "Epoch 18, loss: 2.261443\n",
            "Epoch 19, loss: 2.259647\n",
            "Epoch 20, loss: 2.257910\n",
            "Epoch 21, loss: 2.256237\n",
            "Epoch 22, loss: 2.254527\n",
            "Epoch 23, loss: 2.252871\n",
            "Epoch 24, loss: 2.251254\n",
            "Epoch 25, loss: 2.249659\n",
            "Epoch 26, loss: 2.248050\n",
            "Epoch 27, loss: 2.246511\n",
            "Epoch 28, loss: 2.244924\n",
            "Epoch 29, loss: 2.243497\n",
            "Epoch 30, loss: 2.242023\n",
            "Epoch 31, loss: 2.240594\n",
            "Epoch 32, loss: 2.239173\n",
            "Epoch 33, loss: 2.237835\n",
            "Epoch 34, loss: 2.236448\n",
            "Epoch 35, loss: 2.235084\n",
            "Epoch 36, loss: 2.233805\n",
            "Epoch 37, loss: 2.232505\n",
            "Epoch 38, loss: 2.231232\n",
            "Epoch 39, loss: 2.229942\n",
            "Epoch 40, loss: 2.228719\n",
            "Epoch 41, loss: 2.227516\n",
            "Epoch 42, loss: 2.226322\n",
            "Epoch 43, loss: 2.225199\n",
            "Epoch 44, loss: 2.223985\n",
            "Epoch 45, loss: 2.222915\n",
            "Epoch 46, loss: 2.221793\n",
            "Epoch 47, loss: 2.220699\n",
            "Epoch 48, loss: 2.219583\n",
            "Epoch 49, loss: 2.218562\n",
            "Epoch 50, loss: 2.217528\n",
            "Epoch 51, loss: 2.216514\n",
            "Epoch 52, loss: 2.215433\n",
            "Epoch 53, loss: 2.214502\n",
            "Epoch 54, loss: 2.213537\n",
            "Epoch 55, loss: 2.212513\n",
            "Epoch 56, loss: 2.211614\n",
            "Epoch 57, loss: 2.210673\n",
            "Epoch 58, loss: 2.209783\n",
            "Epoch 59, loss: 2.208844\n",
            "Epoch 60, loss: 2.207982\n",
            "Epoch 61, loss: 2.207114\n",
            "Epoch 62, loss: 2.206295\n",
            "Epoch 63, loss: 2.205393\n",
            "Epoch 64, loss: 2.204543\n",
            "Epoch 65, loss: 2.203752\n",
            "Epoch 66, loss: 2.202928\n",
            "Epoch 67, loss: 2.202138\n",
            "Epoch 68, loss: 2.201356\n",
            "Epoch 69, loss: 2.200599\n",
            "Epoch 70, loss: 2.199802\n",
            "Epoch 71, loss: 2.199057\n",
            "Epoch 72, loss: 2.198380\n",
            "Epoch 73, loss: 2.197624\n",
            "Epoch 74, loss: 2.196880\n",
            "Epoch 75, loss: 2.196185\n",
            "Epoch 76, loss: 2.195459\n",
            "Epoch 77, loss: 2.194804\n",
            "Epoch 78, loss: 2.194189\n",
            "Epoch 79, loss: 2.193443\n",
            "Epoch 80, loss: 2.192860\n",
            "Epoch 81, loss: 2.192166\n",
            "Epoch 82, loss: 2.191517\n",
            "Epoch 83, loss: 2.190907\n",
            "Epoch 84, loss: 2.190304\n",
            "Epoch 85, loss: 2.189687\n",
            "Epoch 86, loss: 2.189087\n",
            "Epoch 87, loss: 2.188493\n",
            "Epoch 88, loss: 2.187916\n",
            "Epoch 89, loss: 2.187293\n",
            "Epoch 90, loss: 2.186719\n",
            "Epoch 91, loss: 2.186166\n",
            "Epoch 92, loss: 2.185632\n",
            "Epoch 93, loss: 2.185050\n",
            "Epoch 94, loss: 2.184539\n",
            "Epoch 95, loss: 2.183961\n",
            "Epoch 96, loss: 2.183419\n",
            "Epoch 97, loss: 2.182855\n",
            "Epoch 98, loss: 2.182385\n",
            "Epoch 99, loss: 2.181908\n",
            "Epoch 100, loss: 2.181384\n",
            "Epoch 101, loss: 2.180874\n",
            "Epoch 102, loss: 2.180405\n",
            "Epoch 103, loss: 2.179916\n",
            "Epoch 104, loss: 2.179444\n",
            "Epoch 105, loss: 2.178952\n",
            "Epoch 106, loss: 2.178505\n",
            "Epoch 107, loss: 2.178083\n",
            "Epoch 108, loss: 2.177554\n",
            "Epoch 109, loss: 2.177118\n",
            "Epoch 110, loss: 2.176716\n",
            "Epoch 111, loss: 2.176248\n",
            "Epoch 112, loss: 2.175814\n",
            "Epoch 113, loss: 2.175400\n",
            "Epoch 114, loss: 2.174946\n",
            "Epoch 115, loss: 2.174550\n",
            "Epoch 116, loss: 2.174138\n",
            "Epoch 117, loss: 2.173707\n",
            "Epoch 118, loss: 2.173271\n",
            "Epoch 119, loss: 2.172892\n",
            "Epoch 120, loss: 2.172565\n",
            "Epoch 121, loss: 2.172103\n",
            "Epoch 122, loss: 2.171694\n",
            "Epoch 123, loss: 2.171303\n",
            "Epoch 124, loss: 2.170895\n",
            "Epoch 125, loss: 2.170575\n",
            "Epoch 126, loss: 2.170190\n",
            "Epoch 127, loss: 2.169823\n",
            "Epoch 128, loss: 2.169454\n",
            "Epoch 129, loss: 2.169083\n",
            "Epoch 130, loss: 2.168764\n",
            "Epoch 131, loss: 2.168343\n",
            "Epoch 132, loss: 2.168075\n",
            "Epoch 133, loss: 2.167743\n",
            "Epoch 134, loss: 2.167333\n",
            "Epoch 135, loss: 2.167003\n",
            "Epoch 136, loss: 2.166651\n",
            "Epoch 137, loss: 2.166337\n",
            "Epoch 138, loss: 2.166015\n",
            "Epoch 139, loss: 2.165683\n",
            "Epoch 140, loss: 2.165373\n",
            "Epoch 141, loss: 2.165016\n",
            "Epoch 142, loss: 2.164719\n",
            "Epoch 143, loss: 2.164366\n",
            "Epoch 144, loss: 2.164097\n",
            "Epoch 145, loss: 2.163771\n",
            "Epoch 146, loss: 2.163444\n",
            "Epoch 147, loss: 2.163136\n",
            "Epoch 148, loss: 2.162830\n",
            "Epoch 149, loss: 2.162563\n",
            "Epoch 150, loss: 2.162267\n",
            "Epoch 151, loss: 2.161986\n",
            "Epoch 152, loss: 2.161744\n",
            "Epoch 153, loss: 2.161390\n",
            "Epoch 154, loss: 2.161052\n",
            "Epoch 155, loss: 2.160788\n",
            "Epoch 156, loss: 2.160557\n",
            "Epoch 157, loss: 2.160215\n",
            "Epoch 158, loss: 2.159941\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 159, loss: 2.159740\n",
            "Epoch 160, loss: 2.159407\n",
            "Epoch 161, loss: 2.159162\n",
            "Epoch 162, loss: 2.158901\n",
            "Epoch 163, loss: 2.158631\n",
            "Epoch 164, loss: 2.158349\n",
            "Epoch 165, loss: 2.158098\n",
            "Epoch 166, loss: 2.157851\n",
            "Epoch 167, loss: 2.157563\n",
            "Epoch 168, loss: 2.157322\n",
            "Epoch 169, loss: 2.157081\n",
            "Epoch 170, loss: 2.156805\n",
            "Epoch 171, loss: 2.156544\n",
            "Epoch 172, loss: 2.156279\n",
            "Epoch 173, loss: 2.156061\n",
            "Epoch 174, loss: 2.155849\n",
            "Epoch 175, loss: 2.155621\n",
            "Epoch 176, loss: 2.155343\n",
            "Epoch 177, loss: 2.155108\n",
            "Epoch 178, loss: 2.154892\n",
            "Epoch 179, loss: 2.154716\n",
            "Epoch 180, loss: 2.154400\n",
            "Epoch 181, loss: 2.154119\n",
            "Epoch 182, loss: 2.153912\n",
            "Epoch 183, loss: 2.153752\n",
            "Epoch 184, loss: 2.153492\n",
            "Epoch 185, loss: 2.153253\n",
            "Epoch 186, loss: 2.153029\n",
            "Epoch 187, loss: 2.152761\n",
            "Epoch 188, loss: 2.152579\n",
            "Epoch 189, loss: 2.152371\n",
            "Epoch 190, loss: 2.152139\n",
            "Epoch 191, loss: 2.151916\n",
            "Epoch 192, loss: 2.151715\n",
            "Epoch 193, loss: 2.151510\n",
            "Epoch 194, loss: 2.151262\n",
            "Epoch 195, loss: 2.151064\n",
            "Epoch 196, loss: 2.150859\n",
            "Epoch 197, loss: 2.150663\n",
            "Epoch 198, loss: 2.150483\n",
            "Epoch 199, loss: 2.150213\n",
            "current accuracy =  0.229\n",
            "----------------------------------\n",
            "Epoch 0, loss: 2.301117\n",
            "Epoch 1, loss: 2.298143\n",
            "Epoch 2, loss: 2.295592\n",
            "Epoch 3, loss: 2.293014\n",
            "Epoch 4, loss: 2.290553\n",
            "Epoch 5, loss: 2.288089\n",
            "Epoch 6, loss: 2.285797\n",
            "Epoch 7, loss: 2.283462\n",
            "Epoch 8, loss: 2.281200\n",
            "Epoch 9, loss: 2.279056\n",
            "Epoch 10, loss: 2.276862\n",
            "Epoch 11, loss: 2.274821\n",
            "Epoch 12, loss: 2.272741\n",
            "Epoch 13, loss: 2.270675\n",
            "Epoch 14, loss: 2.268694\n",
            "Epoch 15, loss: 2.266791\n",
            "Epoch 16, loss: 2.264875\n",
            "Epoch 17, loss: 2.263091\n",
            "Epoch 18, loss: 2.261238\n",
            "Epoch 19, loss: 2.259429\n",
            "Epoch 20, loss: 2.257619\n",
            "Epoch 21, loss: 2.255913\n",
            "Epoch 22, loss: 2.254205\n",
            "Epoch 23, loss: 2.252576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-260-b605b63d6a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                       \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                       reg=reg)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-e4c66862256a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatches_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# add l2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-49bcd154394b>\u001b[0m in \u001b[0;36mlinear_softmax\u001b[0;34m(X, W, target_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXtaKuzARrId",
        "colab_type": "code",
        "colab": {},
        "outputId": "af6bfe6c-c23e-45b2-e8b1-07b2917805f1"
      },
      "source": [
        "print('best validation accuracy achieved: %f' % best_val_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best validation accuracy achieved: 0.251000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaPvslYgRrIn",
        "colab_type": "code",
        "colab": {},
        "outputId": "dfac2c78-bd1d-46e3-834d-a676cffed4cc"
      },
      "source": [
        "test_pred = best_classifier.predict(test_X)\n",
        "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
        "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear softmax classifier test set accuracy: 0.208000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
